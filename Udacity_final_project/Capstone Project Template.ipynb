{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project consists on analysing the data about imigration to the US. To achieve this objective we will export all the data to an S3 bucket so we can import this information to Redshift. Once in Redshift, as staging tables, we will perform a series of transformations in order to produce a schema that we can analyze.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc><br>\n",
    "<br>\n",
    "In this project we are going to use data that, appeared to come from different sources to analyze imigration to the United States. To achieve this goal we will work with a dataset containing information on imigration, another one containing the on airports and the last one containing information on the id available on the imigration dataset. My end solution would be the table schema (Tables_schema.png) upon which I can perform some queries to analyse the information. For this project I decided to make use of python programming language, with its libraries for interacting with AWS Cloud ecosystem (boto3), connecting to Postgres database (psycopg2) among others.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? <br>\n",
    "The datasets used on this project are the ones provided by Udacity's as a project suggestion for completing this course. That are a total of three datasets used. \n",
    "\n",
    "The first one _(`immigration_data_sample.csv`)_ , consists on raw data about imigration. It is consisted mostly on id columns. We can see, just with a quick look that data here must be properly dealt with, because that are some issues like integer information _(such as year and month)_ that are stored as float. At the end, this table was heavily use on our `fact_imigration` table. <br>\n",
    "\n",
    "Another dataset used was the `airport-codes_csv.csv`. It contains detailed information on airports, going from simple information such as name to complex ones like coordinates. Just looking at the top 5 register on the dataset, we can see that NULL values will be a commun issue here. Another interesting column is the coordinate one. In this column we have somthing like an array for each value, refering to lattitude and longitude. In our case, we will not break it into two columns since we are not going to use it in our analysis. Even though, not used, this column will be imported to our staging table.<br>\n",
    "\n",
    "Lastly, we have the `I94_SAS_Labels_Descriptions.SAS` file, which took some effort to treat. For this task I used the work done by a Udacity's student and made some little adjustments to produce a .csv file to be analysed. This data consists basically on a key pair value containing the id and its description for every column on the imigrant tables. This information was used a LOT when producing the dimensions table.<br>\n",
    "\n",
    "The `us-cities-demographics.csv` was not used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_i = pd.read_csv('/home/felipe/Documentos/Udacity/Data Engineering Nanodegree/Udacity_final_project/data/immigration_data_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160423</td>\n",
       "      <td>MTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>DOH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup  \\\n",
       "0      1.0      HI  20573.0    61.0      2.0    1.0  20160422      NaN   NaN   \n",
       "1      1.0      TX  20568.0    26.0      2.0    1.0  20160423      MTR   NaN   \n",
       "2      1.0      FL  20571.0    76.0      2.0    1.0  20160407      NaN   NaN   \n",
       "3      1.0      CA  20581.0    25.0      2.0    1.0  20160428      DOH   NaN   \n",
       "4      3.0      NY  20553.0    19.0      2.0    1.0  20160406      NaN   NaN   \n",
       "\n",
       "  entdepa entdepd  entdepu matflag  biryear   dtaddto gender  insnum airline  \\\n",
       "0       G       O      NaN       M   1955.0  07202016      F     NaN      JL   \n",
       "1       G       R      NaN       M   1990.0  10222016      M     NaN     *GA   \n",
       "2       G       O      NaN       M   1940.0  07052016      M     NaN      LH   \n",
       "3       G       O      NaN       M   1991.0  10272016      M     NaN      QR   \n",
       "4       Z       K      NaN       M   1997.0  07042016      F     NaN     NaN   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  5.658267e+10  00782       WT  \n",
       "1  9.436200e+10  XBLNG       B2  \n",
       "2  5.578047e+10  00464       WT  \n",
       "3  9.478970e+10  00739       B2  \n",
       "4  4.232257e+10   LAND       WT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values _(`immigration_data_sample.csv`)_\n",
    "As we can see below that are a lot of columns with Null values in the imigration dataset.<br>\n",
    "That are some critic cases, which makes it impossible to analyze, such as `occup`, `insnum` and `entdepu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 29 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  1000 non-null   int64  \n",
      " 1   cicid       1000 non-null   float64\n",
      " 2   i94yr       1000 non-null   float64\n",
      " 3   i94mon      1000 non-null   float64\n",
      " 4   i94cit      1000 non-null   float64\n",
      " 5   i94res      1000 non-null   float64\n",
      " 6   i94port     1000 non-null   object \n",
      " 7   arrdate     1000 non-null   float64\n",
      " 8   i94mode     1000 non-null   float64\n",
      " 9   i94addr     941 non-null    object \n",
      " 10  depdate     951 non-null    float64\n",
      " 11  i94bir      1000 non-null   float64\n",
      " 12  i94visa     1000 non-null   float64\n",
      " 13  count       1000 non-null   float64\n",
      " 14  dtadfile    1000 non-null   int64  \n",
      " 15  visapost    382 non-null    object \n",
      " 16  occup       4 non-null      object \n",
      " 17  entdepa     1000 non-null   object \n",
      " 18  entdepd     954 non-null    object \n",
      " 19  entdepu     0 non-null      float64\n",
      " 20  matflag     954 non-null    object \n",
      " 21  biryear     1000 non-null   float64\n",
      " 22  dtaddto     1000 non-null   object \n",
      " 23  gender      859 non-null    object \n",
      " 24  insnum      35 non-null     float64\n",
      " 25  airline     967 non-null    object \n",
      " 26  admnum      1000 non-null   float64\n",
      " 27  fltno       992 non-null    object \n",
      " 28  visatype    1000 non-null   object \n",
      "dtypes: float64(15), int64(2), object(12)\n",
      "memory usage: 226.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_i.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrect Data Types _(`immigration_data_sample.csv`)_\n",
    "Columns that should be presented as integer, such as `i94yr` and `i94mon` are set as float64.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Values _(`immigration_data_sample.csv`)_\n",
    "No duplicates found on the column `cicid`. THis is important because we are going to use it as a key for the future `dim_imgrant` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid\n",
       "6061994.0    1\n",
       "6058513.0    1\n",
       "6057910.0    1\n",
       "6057882.0    1\n",
       "6055844.0    1\n",
       "            ..\n",
       "18310.0      1\n",
       "17786.0      1\n",
       "13826.0      1\n",
       "13213.0      1\n",
       "13208.0      1\n",
       "Name: cicid, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i.groupby(['cicid'])['cicid'].count().sort_index(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weird Column names _(`immigration_data_sample.csv`)_\n",
    "`i94bir` seems to refer to the imigrant's age. Looking at the name of the column, we would think it would refer to something like \"birthday\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61.0\n",
       "1    26.0\n",
       "2    76.0\n",
       "3    25.0\n",
       "4    19.0\n",
       "Name: i94bir, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i.i94bir.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "All the tables used on this project was imported as is to S3 and migrated to Redshift. The issues they presented were dealt with using SQL in Redshift environment. The only exception to this rule is the information contained on the `I94_SAS_Labels_Descriptions.SAS` file. In this case we had to performed the transformations described, and properly documented on the `reading_sas_file.py`. <br>\n",
    "The code on this file was a mix of a code found [here](https://knowledge.udacity.com/questions/125439) and some adjustments I made myself. The result is a .csv file containing all the ids and descriptions for the columns and a final column telling from which column in the imigrant dataset it refers to. This .csv was imported as a staging table to redshift."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Reference for this information comes from [here](https://knowledge.udacity.com/questions/234597)\n",
    "This model is composed of 3 staging tables, 6 dimension tables and 1 fact table.\n",
    "\n",
    "Staging Tables\n",
    "1. staging_imigration: Contains the raw data from the `imigration_data_sample.csv`/parquet\n",
    "2. staging_airport_codes: Contains information on airports\n",
    "3. staging_sas_information: A compilation of ids and descriptions from the columns in the `imigration_data_sample.csv`/parquet\n",
    "\n",
    "Dimension Tables\n",
    "1. dim_modal: Contain information on transport modal ('Air', 'Sea' , 'Land' and 'Not reported');\n",
    "2. dim_port: Contain information on the port of arrival;\n",
    "3. dim_imgrant: Contains information on the imigrant himself/herself;\n",
    "4. dim_country: Contain the name of the country, and its respective id;\n",
    "5. dim_state: Contain the name of the state, and its respective id;\n",
    "6. dim_visa_motive: Contain information on the motive of the visa\n",
    "\n",
    "Fact Table\n",
    "1. fact_imigration: Contains information on the act of imigration itself.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "Since the data was provided by Udacity, we start our pipeline with the files already in possession.<br>\n",
    "- Generate the `sas_descriptive_information.csv` by executing the `reading_sas_file.py`;\n",
    "- Upload all the .csv files into the `nogfel-imigration` S3 bucket, respecting the path below:\n",
    "    - s3://nogfel-imigration/airport_data for `airport-codes_csv.csv`;\n",
    "    - s3://nogfel-imigration/imigration_data for `immigration_data_sample.csv` and;\n",
    "    - s3://nogfel-imigration/sas_data for `sas_descriptive_information.csv`\n",
    "- Execute `create_cluster_aws.py` to create the Redshift cluster and the necessary infrastructure on AWS Cloud;\n",
    "- Execute `create_and_load_tables.py` to create all the tables, and load them, with the data necessary for the analysis;\n",
    "- Execute `quality_checks.py` to perform checks on the data to see if everything is ok."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "All the code necessary are available on the files:\n",
    "- `create_cluster_aws.py`;\n",
    "- `create_and_load_tables.py` and;\n",
    "- `quality_checks.py`\n",
    "\n",
    "THe files must be executed in order above for the correct execution of the job."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Quality check codes available on `quality_checks.py` file.<br><br>\n",
    "<font color='orange'>Insert a picture of the output of the `quality_checks.py` showing that everything is Ok.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "The data dictionary presented here was based on [this](https://knowledge.udacity.com/questions/875627) reference.<br>\n",
    "The data dictionary is available in [this google sheets file](https://docs.google.com/spreadsheets/d/1Y7uEm-tTa66jRgp2h-mtDe9kk_6elhQmDKd_N2US3q8/edit#gid=0). The acces is available, for reading, for anyone who has the link.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project\n",
    "_This project made use of AWS S3 for storaging raw data, Pandas (python's library) for data exploration and Redshift for data wrangling._<br>\n",
    "##### Propose how often the data should be updated and why.\n",
    "_The imigrant's dataset, which was heavily used for generating the fact table, only presented information for one month, April 2016. So, it should be updated monthly. The other two, does not need to update with this frequence. Updates once every 2 or three months will be enough._\n",
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.<br>\n",
    " _In this scenario we could make use of Spark running on a Amazon EMR, a service specific for processing large amounts of data._\n",
    "\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.<br>\n",
    " _Apache Airflow would be the go for product for this scenario. In this situation the whole ETL process created here should be migrated to Airflow to make use of DAGs and the scheduler in order to meet the specification._\n",
    " \n",
    " * The database needed to be accessed by 100+ people.<br>\n",
    " _The project already makes use of Amazon's Redshift Database, which can handle up to 500 connections ([source](https://docs.aws.amazon.com/redshift/latest/mgmt/amazon-redshift-limits.html)) and would be more than enough to handle this demand._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
